{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Utils - Image Detection example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fashion dataset (Kaggle)  https://www.kaggle.com/datafiniti/womens-shoes-prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import pckgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://i5.walmartimages.com/asr/861ca6cf-fa55-4a48-904d-b764d7c00f0c_1.1a2bb39923e1486d05bdafe37ad832e3.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF,https://i5.walmartimages.com/asr/efe9ba1e-daed-4534-9e2e-11804bbb30f1_1.62e3e7f0268f641323a245d5caebdd6d.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF,https://i5.walmartimages.com/asr/0c717815-228e-4c9b-a8fc-d033576461c9_1.f08402e0a5165746e133ddeb589c73e0.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF,https://i5.walmartimages.com/asr/f46703c8-2cdb-4bf2-a3ea-819f24aab134_1.df725b76ca0112d64bdf566ad97760a9.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF,https://i5.walmartimages.com/asr/f652f354-a1fb-47ac-b507-7f97eb216b14_1.39e78b87e2328421803115869ee8b950.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF,http://ak1.ostkcdn.com/images/products/84/146/P16141204.jpg,http://s1.shoes.com/images/br021/womens-naturalizer-danya-soft-silver-crosshatch-shiny-377672_366_tp.jpg,http://s3.shoes.com/images/br021/womens-naturalizer-danya-soft-silver-crosshatch-shiny-377672_366_sl.jpg,http://s2.shoes.com/images/br021/womens-naturalizer-danya-soft-silver-crosshatch-shiny-377672_366_rt.jpg,http://s3.shoes.com/images/br021/womens-naturalizer-danya-soft-silver-crosshatch-shiny-377672_366_45.jpg,http://s2.shoes.com/images/br021/womens-naturalizer-danya-soft-silver-crosshatch-shiny-377672_366_hl.jpg,http://s1.shoes.com/images/br021/womens-naturalizer-danya-soft-silver-crosshatch-shiny-377672_366_lt.jpg,http://ak1.ostkcdn.com/images/products/85/115/P16454357.jpg',\n",
       " 'price': '55.99',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the csv with price and image url\n",
    "import csv\n",
    "\n",
    "with open('data.csv', 'r') as file:\n",
    "    table = csv.reader(file, delimiter=',')\n",
    "    \n",
    "    ## skip headers\n",
    "    next(table, None)\n",
    "    \n",
    "    ## get data (total 10,000 rows)\n",
    "    #data = [{\"id\":i, \"url\":row[10], \"price\":row[16]} for i,row in enumerate(table)]\n",
    "    \n",
    "    ## some items are the same, I'll keep the unique values (653 rows)\n",
    "    lst_id, data = [], []\n",
    "    for row in table:\n",
    "        if row[0] not in lst_id:\n",
    "            lst_id.append(row[0])\n",
    "            data.append({\"url\":row[10], \"price\":row[16]})\n",
    "        else:\n",
    "            next\n",
    "    \n",
    "    ## insert a custom id\n",
    "    for i,dic in enumerate(data):\n",
    "        dic[\"id\"]=i\n",
    "        \n",
    "print(\"len:\", len(data))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 | res: 200\n",
      "384 | res: 200\n"
     ]
    }
   ],
   "source": [
    "# Scraping the imgaes from the web and downloading on file system\n",
    "import requests\n",
    "import os\n",
    "\n",
    "## create the folder if doesn't exist \n",
    "dirpath = \"imgs/\"\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "    \n",
    "## api get each url and save the image into file\n",
    "for dic in data:\n",
    "    try:\n",
    "        res = requests.get(dic[\"url\"])\n",
    "        if res.status_code == 200:\n",
    "            file_name = str(dic[\"id\"])+'.jpg'\n",
    "            file = open(dirpath+file_name,'wb')\n",
    "            file.write(res.content)\n",
    "            file.close()\n",
    "    except:\n",
    "        print(dic[\"id\"], \"| res:\", res.status_code)\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = utils_load_img(dirpath, \"0.jpg\", figsize=(7,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Image Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Look for Metadata\n",
    "- size\n",
    "- colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Baseline (Bag of Words + Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_bow = fit_bow(corpus=dtf_train[\"text_clean\"], vectorizer=vectorizer, top=10, figsize=(10,3))\n",
    "\n",
    "X_train, vectorizer, dic_vocabulary, lst_text2tokens = dic_bow[\"X\"], dic_bow[\"vectorizer\"], dic_bow[\"dic_vocabulary\"], dic_bow[\"lst_text2tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from: \", dtf_train[\"text_clean\"].iloc[0], \"| len:\", len(dtf_train[\"text_clean\"].iloc[0].split()))\n",
    "print(\"to: \", lst_text2tokens[0], \"| len:\", len(lst_text2tokens[0]))\n",
    "print(\"check: \", dtf_train[\"text_clean\"].iloc[0].split()[0], \" -- idx in vocabolary -->\", \n",
    "      dic_vocabulary[dtf_train[\"text_clean\"].iloc[0].split()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dic_features_selection = features_selection(X_train, y=dtf_train[\"y\"], vectorizer_fitted=vectorizer, top=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_X_train = sparse2dtf(X_train, dic_vocabulary, lst_words=dic_features_selection[\"ALL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtf_train = pd.concat([dtf_train, dtf_X_train.set_index(dtf_train.index)], axis=1)\n",
    "\n",
    "print(dtf_train.shape)\n",
    "dtf_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(dtf_test[\"text_clean\"])\n",
    "dtf_X_test = sparse2dtf(X_test, dic_vocabulary, lst_words=dic_features_selection[\"ALL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_test = pd.concat([dtf_test, dtf_X_test.set_index(dtf_test.index)], axis=1)\n",
    "\n",
    "print(dtf_test.shape)\n",
    "dtf_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dtf_train.drop([\"y\",\"text\",\"text_clean\",\"tags\",\"lang\"], axis=1).values\n",
    "y_train = dtf_train[\"y\"].values\n",
    "X_test = dtf_test.drop([\"y\",\"text\",\"text_clean\",\"tags\",\"lang\"], axis=1).values\n",
    "y_test = dtf_test[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_model = ml_text_classif(X_train, y_train, X_test, y_test, preprocessing=False, vectorizer=None, classifier=classifier)\n",
    "predicted_prob, predicted = dic_model[\"predicted_prob\"], dic_model[\"predicted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Model Desing & Testing (pre-trained Embeddings + Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I already have:\n",
    "- dtf_train --> x=text_clean, y must be encoded\n",
    "- NB! I need a new vectorizer cuz the one in Baseline has ngrams=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train, dic_y_mapping = encode_variable(dtf_train, \"y\")\n",
    "print(dic_y_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input for lstm (sequences of tokens)\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=None, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, dic_vocabulary = text2seq(corpus=dtf_train[\"text_clean\"], vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from: \", dtf_train[\"text_clean\"].iloc[0], \"| len:\", len(dtf_train[\"text_clean\"].iloc[0].split()))\n",
    "print(\"to: \", X_train[0], \"| len:\", len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weights for lstm (embeddings of tokens)\n",
    "nlp = gensim_api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vocabulary_embeddings(dic_vocabulary, nlp, dim_space=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = dtf_train[\"text_clean\"].iloc[0].split()[0]\n",
    "print(\"word:\", word)\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \"|vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_w2v(nlp, plot_type=\"2d\", word=word, top=20, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, _ = text2seq(corpus=dtf_test[\"text_clean\"], vectorizer=vectorizer, vocabulary=dic_vocabulary, \n",
    "                     maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dtf_train[\"y\"].values\n",
    "y_test = dtf_test[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "dic_lstm = dl_text_classif(dic_y_mapping, embeddings, X_train, y_train, X_test, y_test, \n",
    "                           model=None, epochs=10, batch_size=256)\n",
    "\n",
    "predicted_prob, predicted = dic_lstm[\"predicted_prob\"], dic_lstm[\"predicted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Model Desing & Testing (training Embeddings from scratch + Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plan:\n",
    "- I will just create a new nlp model using all data\n",
    "- create new Embeddings array with the new nlp model and the same vocabulary\n",
    "- the rest of the process is the same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([len(text.split()) for text in dtf[\"text_clean\"]]) /2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_bigrams_stopwords = [\"of\",\"with\",\"without\",\"and\",\"or\",\"the\",\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_corpus, nlp = fit_w2v(corpus=dtf[\"text_clean\"], ngrams=1, min_count=1, size=300, window=18, sg=0, epochs=30, \n",
    "                          lst_bigrams_stopwords=lst_bigrams_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_w2v(nlp, plot_type=\"2d\", word=word, top=20, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vocabulary_embeddings(dic_vocabulary, nlp, dim_space=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "dic_lstm = dl_text_classif(dic_y_mapping, embeddings, X_train, y_train, X_test, y_test, \n",
    "                           model=None, epochs=10, batch_size=256)\n",
    "\n",
    "predicted_prob, predicted = dic_lstm[\"predicted_prob\"], dic_lstm[\"predicted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Model Desing & Testing (Embedding + Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plan:\n",
    "- I will use the nlp model to create clusters for the classes\n",
    "- then convert news into vectors and calculate distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Create Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the Baseline step\n",
    "dic_clusters = {}\n",
    "for y,lst_keywords in dic_features_selection.items():\n",
    "    if y != \"ALL\":\n",
    "        lst_grams = []\n",
    "        for gram in lst_keywords:\n",
    "            if len(gram.split())>1:\n",
    "                lst_grams.append(\"_\".join(gram.split()))\n",
    "            else:\n",
    "                lst_grams.append(gram)\n",
    "        dic_clusters.update({y:lst_grams})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in dic_clusters.items():\n",
    "    print(k, \": \", v[0:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = fit_pca_w2v(corpus=dtf_train[\"text_clean\"], nlp=nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predict Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prob, predicted = predict_clusters_w2v(corpus=dtf_test[\"text_clean\"], dic_clusters=dic_clusters, \n",
    "                                                 nlp=nlp, pca=None)\n",
    "\n",
    "print(\"Accuracy (overall correct predictions):\",  round(metrics.accuracy_score(y_test, predicted),3))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
